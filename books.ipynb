{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import requests\n",
    "import html\n",
    "import datetime\n",
    "import json\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections.abc import Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.sparse import save_npz, load_npz, hstack, vstack\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df = pd.read_csv('books.csv', sep=\";\", error_bad_lines=False, encoding=\"latin-1\")\n",
    "ratings_df = pd.read_csv('ratings.csv', sep=\";\", error_bad_lines=False, encoding=\"latin-1\")\n",
    "users_df = pd.read_csv('users.csv', sep=\";\", error_bad_lines=False, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(books_df.shape)\n",
    "print(ratings_df.shape)\n",
    "print(users_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration books data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unnecessary columns dropped\n",
    "books_df = books_df.drop(columns=['Image-URL-S', 'Image-URL-M', 'Image-URL-L'])\n",
    "books_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for check if ISBN are valid, defined with regex isbn10 and isbn13 and created a function to detect suspicious ISBNs\n",
    "def is_valid_isbn(isbn):\n",
    "    isbn = re.sub(r'[-\\s]', '', isbn)\n",
    "    isbn_10_pattern = re.compile(r'^\\d{9}[\\dXx]$')\n",
    "    isbn_13_pattern = re.compile(r'^\\d{13}$')\n",
    "    return bool(isbn_10_pattern.match(isbn)) or bool(isbn_13_pattern.match(isbn))\n",
    "\n",
    "invalid_isbn_books = books_df[~books_df['ISBN'].apply(is_valid_isbn)]\n",
    "invalid_isbn_books "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df = books_df[books_df['ISBN'].apply(is_valid_isbn)].reset_index(drop=True)\n",
    "books_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df['Year-Of-Publication'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some suspicious Year of publication as 0 or higher than current year, decided to replace the value with none\\\n",
    "Invalid Year as DK Publishing Inc and Gallimard needs to be shift to publisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_year_rows = books_df[~books_df['Year-Of-Publication'].astype(str).str.isnumeric()]\n",
    "invalid_year_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indxtoshift = invalid_year_rows.index\n",
    "for idx in indxtoshift:\n",
    "    books_df.at[idx, 'Publisher'] = books_df.at[idx, 'Year-Of-Publication']\n",
    "    books_df.at[idx, 'Year-Of-Publication'] = books_df.at[idx, 'Book-Author']\n",
    "    books_df.at[idx, 'Book-Author'] = None \n",
    "books_df.loc[indxtoshift]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The column has combination of str, and int, decided to convert all the value to int \n",
    "books_df['Year-Of-Publication'] = pd.to_numeric(books_df['Year-Of-Publication'], errors='coerce') \n",
    "books_df['Year-Of-Publication'] = books_df['Year-Of-Publication'].astype(pd.Int64Dtype())\n",
    "\n",
    "# Replacing value that are equal to 0 or higher than curren year with na\n",
    "books_df.loc[books_df['Year-Of-Publication'] == 0, 'Year-Of-Publication'] = pd.NA\n",
    "books_df.loc[books_df['Year-Of-Publication'] > int(datetime.date.today().strftime('%Y')), 'Year-Of-Publication'] = pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df['author_name_length'] = books_df['Book-Author'].astype(str).apply(len)\n",
    "sorted_books_df = books_df.sort_values(by='author_name_length', ascending=False)\n",
    "sorted_books_df[['Book-Author', 'author_name_length']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df.loc[219783][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns Book-Author, Book-Title and Publisher entries will need to be cleaned of whitespace, Html entities etc - this will be done later after the datasets have been merged.d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleted unnecessary column created in the above cell\n",
    "books_df = books_df.drop(columns=['author_name_length'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration ratings data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df[~ratings_df['ISBN'].apply(is_valid_isbn)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checked ISBN in ratings df and there is 10159 rows with non valid ISBN, decided to drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df = ratings_df[ratings_df['ISBN'].apply(is_valid_isbn)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df[\"Book-Rating\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration users dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df['Age'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum and minimum of the values (0 and 244) in the age column indicate that the age column contains incorrect values. I decided to keep only the age range from 6 years to 99 and replaced the other values with nan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(users_df['Age'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df['Age'] = users_df['Age'].astype(pd.Int64Dtype())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df['Age'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df.loc[(users_df['Age'] < 6) | (users_df['Age'] > 99), 'Age'] = np.nan\n",
    "users_df['Age'] = users_df['Age'].astype(pd.Int64Dtype())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df[users_df['Age'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently ~40% do not seem to have age information filled in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge data frames and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if there are white spaces, HTML entits and HTML tags or double spaces\n",
    "def analyze_text_issues(text):\n",
    "    double_space = bool(re.search(r'  ', text))\n",
    "    html_tag = bool(re.search(r'<.*?>', text))\n",
    "    html_entity = text != html.unescape(text)\n",
    "    extra_whitespace = text != text.strip()\n",
    "\n",
    "    return double_space, html_tag, html_entity, extra_whitespace\n",
    "\n",
    "# Function which use function analyze_text_issues in columns to display a summary of the issue in columns\n",
    "def summarize_column_issues(df, columns_to_inspect):\n",
    "    summary = {}\n",
    "    for column in columns_to_inspect:\n",
    "        results = df[column].astype(str).dropna().apply(analyze_text_issues)\n",
    "        summary[column] = {\n",
    "            'double_spaces': results.apply(lambda x: x[0]).sum(),\n",
    "            'html_tags': results.apply(lambda x: x[1]).sum(),\n",
    "            'html_entities': results.apply(lambda x: x[2]).sum(),\n",
    "            'extra_whitespace': results.apply(lambda x: x[3]).sum()\n",
    "        }\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean data from white spaces, HTML entits and HTML tags or double spaces\n",
    "def clean_text_data(text):\n",
    "    previous_text = \"\"\n",
    "    while previous_text != text:\n",
    "        previous_text = text\n",
    "        text = re.sub(r'<.*?>', '', text)\n",
    "        text = text.replace('  ', ' ')\n",
    "        text = html.unescape(text)\n",
    "        text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge user and ratings info to get one dataframe with user-ratings info.\n",
    "user_ratings_df = pd.merge(ratings_df, users_df, on='User-ID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe user_ratings_df grouped by ISBN and aggregating the information about age, book rating and user-id in list\n",
    "# The final merge contains the main book dataframe with information about rating, age and user- id aggregated in list\n",
    "isbn_user_ratings = user_ratings_df.groupby('ISBN').agg({\n",
    "    'User-ID': list,\n",
    "    'Book-Rating': list,\n",
    "    'Age': list \n",
    "})\n",
    "merged_df = pd.merge(books_df, isbn_user_ratings, on='ISBN', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_check = ['Book-Title', 'Book-Author', 'Publisher']\n",
    "summary = summarize_column_issues(merged_df, columns_to_check)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function clean_text_data on columns 'Book-Title', 'Book-Author', 'Publisher'\n",
    "for column in ['Book-Title', 'Book-Author', 'Publisher']:\n",
    "    merged_df[column] = merged_df[column].astype(str).apply(clean_text_data)\n",
    "merged_df[['Book-Title', 'Book-Author', 'Publisher']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several columns have empty values, for example Year-Of-Publication, Publisher can be checked to get data from the API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a list of query where each element has 100 unique ISBN, which meets the conditions of the API.\n",
    "def create_list_query(merged_df):\n",
    "    unique_isbn = merged_df['ISBN'].unique()\n",
    "    isbn_list = unique_isbn.tolist()\n",
    "    chunked_isbn_list = [isbn_list[i:i + 100] for i in range(0, len(isbn_list), 100)]\n",
    "    list_query = []\n",
    "    for chunk in chunked_isbn_list:\n",
    "        isbn_str = \",\".join(f\"ISBN:{isbn}\" for isbn in chunk)\n",
    "        url = f\"https://openlibrary.org/api/books?bibkeys={isbn_str}&jscmd=details&format=json\"\n",
    "        list_query.append(url)\n",
    "    return list_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to get a list of query \n",
    "# list_query = create_list_query(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get data and continuosly save them in a new line in data.json file, if error will occure it will give the url and status code\n",
    "def get_data(list_query):\n",
    "    with open('data.json', 'a') as output_file:  \n",
    "        for url in list_query:\n",
    "                response = requests.get(url)\n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    json.dump(data, output_file)\n",
    "                    output_file.write('\\n')  # Newline character to separate JSON objects\n",
    "                else:\n",
    "                    print(f\"Request failed for {url}. Status code: {response.status_code}\")\n",
    "                time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to get data from API\n",
    "# get_data(list_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get data from json file obtained from api and transform them to a dataframe\n",
    "def extract_book_data(file_path):\n",
    "    all_books = []\n",
    "    # open the file and iterate each line and parsing the json object\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            json_obj = json.loads(line.strip())\n",
    "            # for the json object get the information for ISBN\n",
    "            for isbn, book_info in json_obj.items():\n",
    "                if isinstance(book_info, dict):\n",
    "                    details = book_info.get('details', {})\n",
    "                    \n",
    "                    if isinstance(details, dict):\n",
    "                        title = details.get('title')\n",
    "                        number_of_pages = details.get('number_of_pages')\n",
    "                        publish_date = details.get('publish_date')\n",
    "\n",
    "                        publishers = details.get('publishers', [])\n",
    "                        publisher = publishers[0] if publishers else None\n",
    "                        \n",
    "                        if isinstance(details.get('subjects', []), list):\n",
    "                            subjects = details.get('subjects', [])\n",
    "                        else:\n",
    "                            subjects = []\n",
    "\n",
    "                        if isinstance(details.get('genres', []), list):\n",
    "                            genres = details.get('genres', [])\n",
    "                        else:\n",
    "                            genres = []\n",
    "\n",
    "                        book_data = {\n",
    "                            'ISBN': isbn,\n",
    "                            'Title': title,\n",
    "                            'Number_of_Pages': number_of_pages,\n",
    "                            'Publisher': publisher,\n",
    "                            'publish_date': publish_date,\n",
    "                            'Subjects': ', '.join(subjects), \n",
    "                            'Genres': ', '.join(genres) \n",
    "                        }\n",
    "                        all_books.append(book_data)\n",
    "\n",
    "    return pd.DataFrame(all_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_api = 'data.json' \n",
    "external_books_df = extract_book_data(new_data_api)\n",
    "external_books_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removed ISBN: in columns ISBN\n",
    "external_books_df['ISBN'] = external_books_df['ISBN'].str.replace('ISBN:', '', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page number from float to int\n",
    "external_books_df['Number_of_Pages'] = external_books_df['Number_of_Pages'].astype(pd.Int64Dtype())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_check = ['Title', 'Subjects', 'Publisher', 'Genres']\n",
    "summary = summarize_column_issues(external_books_df, columns_to_check)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ['Title', 'Subjects', 'Publisher', 'Genres']:\n",
    "    external_books_df[column] = external_books_df[column].astype(str).apply(clean_text_data)\n",
    "external_books_df[['Title', 'Subjects', 'Publisher', 'Genres']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'publish_date' column to datetime and keeping only the information about year\n",
    "external_books_df['publish_date'] = pd.to_datetime(external_books_df['publish_date'], errors='coerce')\n",
    "external_books_df['year'] = external_books_df['publish_date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_books_df['year'] = external_books_df['publish_date'].dt.year\n",
    "\n",
    "# Drop the original 'publish_date' column as we now have the year\n",
    "external_books_df = external_books_df.drop('publish_date', axis=1)\n",
    "\n",
    "# Return the first few rows of the modified DataFrame to check the result\n",
    "external_books_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting year value to int\n",
    "external_books_df['year'] = external_books_df['year'].astype(pd.Int64Dtype())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling information about year which is present in external_book_df and missing in merged_df\n",
    "# It filled 3574 values\n",
    "comparison_df = pd.merge(merged_df[['ISBN', 'Year-Of-Publication']], external_books_df[['ISBN', 'year']], on='ISBN', how='outer', indicator=True)\n",
    "condition = (comparison_df['Year-Of-Publication'].isna()) & (comparison_df['year'].notna())\n",
    "isbns_to_update = comparison_df.loc[condition, 'ISBN']\n",
    "for isbn in isbns_to_update:\n",
    "    new_year = comparison_df.loc[comparison_df['ISBN'] == isbn, 'year'].iloc[0]\n",
    "    merged_df.loc[merged_df['ISBN'] == isbn, 'Year-Of-Publication'] = new_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_genres = external_books_df['Genres'].unique()\n",
    "unique_genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing text \"etc., etc and .\", double spaces \n",
    "def clean_and_split_genres(genre):\n",
    "    if pd.isna(genre):\n",
    "        return genre \n",
    "    \n",
    "    cleaned_genre = re.sub(r'\\.', '', genre)\n",
    "    cleaned_genre = re.sub(r'\\betc\\b\\.?', '', cleaned_genre)\n",
    "    cleaned_genre = re.sub(r'\\s+', ' ', cleaned_genre)\n",
    "    split_genres = [g.strip() for g in cleaned_genre.split(',') if g.strip()]\n",
    "    \n",
    "    return split_genres\n",
    "external_books_df['Genres'] = external_books_df['Genres'].apply(clean_and_split_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_genres_count = external_books_df[external_books_df['Genres'].apply(lambda x: not x)].shape[0]\n",
    "empty_genres_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunatelly there are so many empty list for column 'Genres', that this columns cannot be used in this form.\\\n",
    "The \"Subject\" column contains many keywords that can be considered a category and help recommend books. In the next steps I will try to extract them and extend category column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Subjects' entries into lists so it will be possible to apply function clean_and_split_list\n",
    "external_books_df['Subjects'] = external_books_df['Subjects'].apply(lambda x: [x] if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_split_list(text_list):\n",
    "    # Initialize an empty list to store cleaned words\n",
    "    cleaned_words = []\n",
    "\n",
    "    # Check if the list is not empty or null\n",
    "    if text_list and isinstance(text_list, list):\n",
    "        for text in text_list:\n",
    "            if pd.isna(text) or not isinstance(text, str):\n",
    "                continue \n",
    "\n",
    "            text = text.lower()\n",
    "            cleaned_text = re.sub(r'[^a-z0-9\\s-]', ' ', text)  \n",
    "            cleaned_text = re.sub(r'(?<!\\w)-|-(?!\\w)', ' ', cleaned_text)\n",
    "            cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "\n",
    "            word_list = cleaned_text.split()\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            word_list = [word for word in word_list if word not in stop_words]\n",
    "\n",
    "            # Extend the main list with the cleaned words\n",
    "            cleaned_words.extend(word_list)\n",
    "\n",
    "    return cleaned_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to both 'Subjects' and 'Genres' columns\n",
    "external_books_df['Cleaned_Subjects'] = external_books_df['Subjects'].apply(clean_and_split_list)\n",
    "external_books_df['Cleaned_Genres'] = external_books_df['Genres'].apply(clean_and_split_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenated column Cleaned_Subjects and Cleaned_Genres in one list\n",
    "external_books_df['Concatenated_S_G'] = external_books_df['Cleaned_Subjects'] + external_books_df['Cleaned_Genres']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to lemmatize text\n",
    "def lemmatize_words(word_list):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # Lemmatize each word in the list\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in word_list]\n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the function to lemmatize text do the column Concatenated_S_G\n",
    "external_books_df['Lemmatized_S_G'] = external_books_df['Concatenated_S_G'].apply(lemmatize_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to check which genres are in our data to specify the known genres \n",
    "# non_empty_genres = external_books_df['Cleaned_Genres'].dropna().loc[external_books_df['Cleaned_Genres'] != '']\n",
    "# unique_genres_set = set()\n",
    "# for genre_list in external_books_df['Cleaned_Genres']:\n",
    "#     if genre_list:  \n",
    "#         unique_genres_set.update(genre_list)\n",
    "# print(list(unique_genres_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_genres = [\n",
    "    'adventure', 'biography', 'fantasy', 'historical', 'horror', 'literary', 'mystery', \n",
    "    'mythology', 'non-fiction', 'philosophical', 'romance', 'satire', 'science', 'juvenile',\n",
    "    'thriller', 'western', 'young', 'action', 'drama', 'erotica', 'memoir', 'crime', \n",
    "    'dystopian', 'self-help', 'travel', 'guide', 'anthology', 'classic', 'comedy', \n",
    "    'psychological', 'suspense', 'tragedy', 'fairy', 'folklore', 'legend', 'narrative', \n",
    "    'periodical', 'political', 'realistic', 'reference', 'religion', 'short', 'superhero', \n",
    "    'supernatural', 'textbook', 'urban', 'utopian', 'war', 'absurdist', 'alternate', \n",
    "    'coming-of-age', 'cookbook', 'diary', 'encyclopedia', 'epic', 'experimental', 'fable', \n",
    "    'fan', 'gothic', 'graphic', 'hard-boiled', 'historiography', 'humor', 'lab', 'magical', \n",
    "    'paranormal', 'picaresque', 'post-apocalyptic', 'stream-of-consciousness', 'sword', \n",
    "    'true', 'vampire', 'visionary', 'whodunit', 'non-fiction', 'biology', 'music', 'guidebook',\n",
    "    'vocabularies', 'design', 'architecture', 'novela', 'archeology', 'tour', 'statistic',\n",
    "    'anecdotes', 'guidebook', 'manual', 'history', 'child', 'study', 'work', 'dictionaries', 'humor',\n",
    "    'handbook', 'pictorial', 'personal', 'poetry', 'interview', 'fiction', 'literature', 'guidebooks',\n",
    "    'social', 'detective', 'life', 'fictitious', 'art'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check for matched from list know_genres in df with list of known genres\n",
    "def find_genre_matches(lemmatized_genres, known_genres):\n",
    "    match = [genre for genre in known_genres if genre in lemmatized_genres]\n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created new column Category where are saved Category of book in list applying function find_genre_matches\n",
    "external_books_df['Category'] = external_books_df['Lemmatized_S_G'].apply(lambda row: find_genre_matches(row, known_genres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_books_df['Category'].apply(lambda x: len(x) == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "269364 - 117928\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(100/269364) * 151436"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_books_df.to_csv(\"test_external_books_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have created a new column named 'Category' that contains more accurate information regarding the categories of various books, presented in list format. This enhancement came after cleaning the 'Subject' and 'Genres' columns, followed by word splitting and the application of lemmatization techniques using a library. Subsequently, I established a list of prevalent book categories. Upon cross-verifying the presence of these categories within the text, I generated a new column. This column boasts a fill rate of approximately 56%. This new column will be used for book recommendation model more accorated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final processing of merged dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a new column for Publication Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new column named 'Publication-Range' whith a range of Year-Of-Publication by decade\n",
    "bins = list(range(1900, 2030, 10))  \n",
    "labels = [f\"{i}-{i+10}\" for i in bins[:-1]]\n",
    "\n",
    "merged_df['Publication-Range'] = pd.cut(merged_df['Year-Of-Publication'], bins=bins, labels=labels, right=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a new column mean rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert Book-Rating to list\n",
    "def convert_to_list(rating):\n",
    "    if isinstance(rating, list):\n",
    "        return rating\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['Book-Rating'] = merged_df['Book-Rating'].apply(convert_to_list)\n",
    "\n",
    "# Calculated the mean rating, taking into account empty lists and ensuring division by zero doesn't occur\n",
    "merged_df['Mean-Rating'] = merged_df['Book-Rating'].apply(lambda x: sum(x) / len(x) if x else float('nan'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the entire 'Mean-Rating' column to native Python data types\n",
    "merged_df['Mean-Rating'] = merged_df['Mean-Rating'].apply(lambda x: int(x) if isinstance(x, float) and x.is_integer() else x).astype(object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join column from external_books_df to final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extented merged_df of external_book_df on ISBN by selected column\n",
    "final_merge = pd.merge(merged_df, external_books_df[['ISBN','Category','Number_of_Pages']], on='ISBN', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unnecessary columns dropped\n",
    "final_merge = final_merge.drop(columns=['Year-Of-Publication'])\n",
    "final_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dealing with < NA > values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_na(value):\n",
    "    return None if pd.isna(value) else value\n",
    "\n",
    "final_merge[\"Age\"] = final_merge[\"Age\"].apply(lambda x: [replace_na(item) for item in x] if isinstance(x, list) else replace_na(x))\n",
    "final_merge[\"Number_of_Pages\"] = final_merge[\"Number_of_Pages\"].apply(lambda x: [replace_na(item) for item in x] if isinstance(x, list) else replace_na(x))\n",
    "final_merge[\"Book-Rating\"] = final_merge[\"Book-Rating\"].apply(lambda x: [replace_na(item) for item in x] if isinstance(x, list) else replace_na(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book Recommendation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create new dataframe by filtering rows that contain the user_id in the 'User-ID' lists.\n",
    "def get_reviewed_by_user(user_id, df):\n",
    "    df_reviewed_by_user = df[df['User-ID'].apply(lambda x: user_id in x if isinstance(x, list) else False)]\n",
    "    return df_reviewed_by_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_book_details(df):\n",
    "    columns_of_interest = ['ISBN', 'Publication-Range', 'Category', 'Book-Author', 'Mean-Rating', 'Publisher']\n",
    "    df_books_details = df[columns_of_interest].drop_duplicates(subset='ISBN')\n",
    "    return df_books_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get a dataframe of ISBN, User-ID and Book-Rating exploding the data from the function get_user_isbns\n",
    "def get_user_ratings(user_data, user_id):\n",
    "    user_data_exploded = user_data.explode('User-ID').reset_index(drop=True)\n",
    "    user_data_exploded['Book-Rating'] = user_data['Book-Rating'].explode().reset_index(drop=True)\n",
    "\n",
    "    filtered_data = user_data_exploded[user_data_exploded['User-ID'] == user_id]\n",
    "    user_rating = filtered_data[['ISBN', 'User-ID', 'Book-Rating']]\n",
    "    return user_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_users_books(user_rating, book_details):\n",
    "    \n",
    "    high_rated_books = user_rating[user_rating['Book-Rating'] >= 6]['ISBN']\n",
    "    low_rated_books = user_rating[user_rating['Book-Rating'] < 5]['ISBN']\n",
    "\n",
    "    high_rating = book_details[book_details['ISBN'].isin(high_rated_books)]\n",
    "    low_rating = book_details[book_details['ISBN'].isin(low_rated_books)]\n",
    "    return high_rating, low_rating  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two primary recommendation techniques to consider: Content-Based Recommendation and Collaborative Filtering. Given that our dataset lacks a substantial number of reviews, Collaborative Filtering may not be the most appropriate choice. Consequently, the decision has been made to proceed with the Content-Based Recommendation approach. \n",
    "First, features will be set which represent the profile of the book and will assist in the decision-making for the recommendation algorithm. Consequently, 'Category', 'Publisher', and 'Book-Author' will be extracted into a new dataframe. This information needs to be encoded, and given the considerable size of the dataset, one-hot encoding is deemed unsuitable. Feature hashing is considered to be a solution. The defined categorical features will be transformed into a numerical format, ensuring that the data becomes suitable for the model. Due to the size of the dataset, feature hashing will be processed in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation_data = final_merge.copy()\n",
    "recommendation_data = recommendation_data[['Category', 'Publisher', 'Book-Author']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all numeric values to strings and handle null values\n",
    "recommendation_data['Category'] = recommendation_data['Category'].apply(lambda x: str(x) if not isinstance(x, str) else x).fillna('Unknown')\n",
    "recommendation_data['Publisher'] = recommendation_data['Publisher'].apply(lambda x: str(x) if not isinstance(x, str) else x).fillna('Unknown')\n",
    "recommendation_data['Book-Author'] = recommendation_data['Book-Author'].apply(lambda x: str(x) if not isinstance(x, str) else x).fillna('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the size of the hash space and initializing the FeatureHasher\n",
    "n_features = 2**17 \n",
    "hasher = FeatureHasher(n_features=n_features, input_type='string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directory to save batches, first it checks if the directory exists, and if not, it creates it\n",
    "directory_path = 'hashed_batches' \n",
    "if not os.path.exists(directory_path):\n",
    "    os.makedirs(directory_path)\n",
    "\n",
    "# Set the batch size\n",
    "batch_size = 500  # Or another size fitting your memory capacity\n",
    "n_batches = len(recommendation_data) // batch_size + 1\n",
    "\n",
    "hashed_results = []\n",
    "\n",
    "\"The for-loop processes recommendation_data in batches. Within each batch, string values are converted to lists, which are then transformed using a hashing function.\"\n",
    "\"These hashed features are horizontally combined to form a single feature matrix for the batch.\"\n",
    "\"This matrix is then saved to disk as a sparse matrix, ensuring each saved batch retains the same number of rows as in the original dataset.\"\n",
    "\n",
    "for i in range(n_batches):\n",
    "    start_index = i * batch_size\n",
    "    end_index = (i + 1) * batch_size\n",
    "\n",
    "    batch = recommendation_data[start_index:end_index]\n",
    "\n",
    "    categories_iterable = [[item] for item in batch['Category'].tolist()]\n",
    "    publishers_iterable = [[item] for item in batch['Publisher'].tolist()]\n",
    "    authors_iterable = [[item] for item in batch['Book-Author'].tolist()]\n",
    "\n",
    "    # Apply hashing for each batch and keep the results as sparse matrices\n",
    "    hashed_category = hasher.transform(categories_iterable)\n",
    "    hashed_publisher = hasher.transform(publishers_iterable)\n",
    "    hashed_author = hasher.transform(authors_iterable)\n",
    "\n",
    "    # Combine hashed features for the batch (in sparse matrix format)\n",
    "    combined_features_batch = hstack([hashed_category, hashed_publisher, hashed_author]) \n",
    "\n",
    "    # Save each batch to disk instead of keeping in memory\n",
    "    batch_file_path = os.path.join(directory_path, f'batch_{i}.npz')  # safer with os.path.join\n",
    "    save_npz(batch_file_path, combined_features_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Used to load previously saved batches of hashed data from disk, collect them into a list, and then concatenate them to produce a single combined sparse matrix of features\"\n",
    "\n",
    "# Set the batch size\n",
    "batch_size = 500 \n",
    "directory_path = 'hashed_batches' \n",
    "n_batches = len(recommendation_data) // batch_size + 1\n",
    "\n",
    "hashed_results = []\n",
    "\n",
    "# Load all batches\n",
    "for i in range(n_batches):\n",
    "    batch_file_path = os.path.join(directory_path, f'batch_{i}.npz')\n",
    "    batch_data = load_npz(batch_file_path)\n",
    "    hashed_results.append(batch_data)\n",
    "\n",
    "# Concatenate all batches to get the final feature set (as a sparse matrix)\n",
    "combined_features_sparse = vstack(hashed_results)  # This is still a sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_features_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimension of the sparse metrix has 271243 rows and 393216 columns. Where each row represents a unique book and each column represents a unique feature that has been hashed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_books(book_id, X, book_ids, k, metric='cosine', show_distance=False):\n",
    "    \"\"\"\n",
    "    Find k similar books based on their vector representation in X.\n",
    "    \n",
    "    Parameters:\n",
    "    - book_id: The ID (ISBN) of the book of interest.\n",
    "    - X: The sparse metrix representation of the books.\n",
    "    - book_ids: A list of book IDs in the same order as they appear in X.\n",
    "    - k: Number of similar books to find.\n",
    "    - metric: The distance metric to use. Default is 'cosine'.\n",
    "    - show_distance: Whether to show distance values. Default is False.\n",
    "    \n",
    "    Returns:\n",
    "    A list of k book IDs that are similar to the given book_id.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find the index of the book_id in the book_ids list\n",
    "    book_ind = book_ids.index(book_id)\n",
    "    \n",
    "    # Extract the vector representation of the book from X\n",
    "    book_vec = X[book_ind]\n",
    "    \n",
    "    # Initialize and fit the kNN model\n",
    "    kNN = NearestNeighbors(n_neighbors=k+1, algorithm=\"brute\", metric=metric)\n",
    "    kNN.fit(X)\n",
    "    \n",
    "    # Reshape the book vector and find its neighbors\n",
    "    book_vec = book_vec.reshape(1, -1)\n",
    "    neighbours = kNN.kneighbors(book_vec, return_distance=show_distance)\n",
    "    \n",
    "    # Extract the indices of the neighbours from the kNN result\n",
    "    neighbour_indices = neighbours[0].tolist()\n",
    "    \n",
    "    # Convert these indices back to book IDs\n",
    "    neighbour_ids = [book_ids[i] for i in neighbour_indices]\n",
    "    \n",
    "    # Remove the original book_id from the result\n",
    "    neighbour_ids.remove(book_id)\n",
    "\n",
    "    return neighbour_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the user_id for which to get the book recommendation\n",
    "user_id = 8\n",
    "\n",
    "df_reviewed_by_user = get_reviewed_by_user(user_id, final_merge)\n",
    "df_books_details = extract_book_details(df_reviewed_by_user)\n",
    "user_rating = get_user_ratings(df_reviewed_by_user, user_id)\n",
    "high_rating, low_rating  = categorize_users_books(user_rating, df_books_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of highly rated books for a selected user\n",
    "book_ids = high_rating['ISBN']\n",
    "book_ids.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isbns = final_merge['ISBN']\n",
    "isbn_list = isbns.to_list()\n",
    "#book_id = \"0002005018\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sorted data frame of books based on their average rating and the number of reviews they have received\n",
    "# Will be used for recommending books to users who have not yet rated any books\n",
    "\n",
    "final_merge['length'] = final_merge['Book-Rating'].apply(len)\n",
    "sorted_df = final_merge.sort_values(by=['Mean-Rating', 'length'], ascending=[False, False])\n",
    "sorted_df = sorted_df.drop(columns=['length'])\n",
    "\n",
    "sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get recommendation for books. It checks if a user has rated any books.\n",
    "# If they haven't, it provides general recommendations based on the top-rated books.\n",
    "# If they have, it offers personalized recommendations based on the user's high-rated books.\n",
    "\n",
    "def get_recommendation(user_high_rating_list, general_recommended_books):\n",
    "    all_books = []\n",
    "    if len(user_high_rating_list) == 0:\n",
    "        \n",
    "        all_books = general_recommended_books['ISBN'].head(3).tolist()\n",
    "    else:    \n",
    "        all_books = all_books + find_similar_books(isbn, combined_features_sparse, isbn_list, 5)\n",
    "    return all_books        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_books = get_recommendation(book_ids, sorted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_books_information = final_merge[final_merge['ISBN'].isin(found_books)]\n",
    "found_books_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final recommended book for the set user\n",
    "recommended_books = found_books_information.sort_values(by='Mean-Rating', ascending=False).head(3)\n",
    "recommended_books"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
