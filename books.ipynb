{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import requests\n",
    "import html\n",
    "import datetime\n",
    "import json\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections.abc import Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df = pd.read_csv('books.csv', sep=\";\", error_bad_lines=False, encoding=\"latin-1\")\n",
    "ratings_df = pd.read_csv('ratings.csv', sep=\";\", error_bad_lines=False, encoding=\"latin-1\")\n",
    "users_df = pd.read_csv('users.csv', sep=\";\", error_bad_lines=False, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(books_df.shape)\n",
    "print(ratings_df.shape)\n",
    "print(users_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration books data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unnecessary columns dropped\n",
    "books_df = books_df.drop(columns=['Image-URL-S', 'Image-URL-M', 'Image-URL-L'])\n",
    "books_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for check if ISBN are valid, defined with regex isbn10 and isbn13 and created a function to detect suspicious ISBNs\n",
    "def is_valid_isbn(isbn):\n",
    "    isbn = re.sub(r'[-\\s]', '', isbn)\n",
    "    isbn_10_pattern = re.compile(r'^\\d{9}[\\dXx]$')\n",
    "    isbn_13_pattern = re.compile(r'^\\d{13}$')\n",
    "    return bool(isbn_10_pattern.match(isbn)) or bool(isbn_13_pattern.match(isbn))\n",
    "\n",
    "invalid_isbn_books = books_df[~books_df['ISBN'].apply(is_valid_isbn)]\n",
    "invalid_isbn_books "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df = books_df[books_df['ISBN'].apply(is_valid_isbn)].reset_index(drop=True)\n",
    "books_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df['Year-Of-Publication'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some suspicious Year of publication as 0 or higher than current year, decided to replace the value with none\\\n",
    "Invalid Year as DK Publishing Inc and Gallimard needs to be shift to publisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_year_rows = books_df[~books_df['Year-Of-Publication'].astype(str).str.isnumeric()]\n",
    "invalid_year_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indxtoshift = invalid_year_rows.index\n",
    "for idx in indxtoshift:\n",
    "    books_df.at[idx, 'Publisher'] = books_df.at[idx, 'Year-Of-Publication']\n",
    "    books_df.at[idx, 'Year-Of-Publication'] = books_df.at[idx, 'Book-Author']\n",
    "    books_df.at[idx, 'Book-Author'] = None \n",
    "books_df.loc[indxtoshift]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The column has combination of str, and int. Converting all the value to int \n",
    "books_df['Year-Of-Publication'] = pd.to_numeric(books_df['Year-Of-Publication'], errors='coerce') \n",
    "books_df['Year-Of-Publication'] = books_df['Year-Of-Publication'].astype(pd.Int64Dtype())\n",
    "\n",
    "# Replacing value that are equal to 0 or higher than curren year with na\n",
    "books_df.loc[books_df['Year-Of-Publication'] == 0, 'Year-Of-Publication'] = pd.NA\n",
    "books_df.loc[books_df['Year-Of-Publication'] > int(datetime.date.today().strftime('%Y')), 'Year-Of-Publication'] = pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df['author_name_length'] = books_df['Book-Author'].astype(str).apply(len)\n",
    "sorted_books_df = books_df.sort_values(by='author_name_length', ascending=False)\n",
    "sorted_books_df[['Book-Author', 'author_name_length']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df.loc[219783][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Book-Author, Book-Title and Publisher will need to be cleaned of whitespace, Html entities etc - this will be done later once the datasets have been merged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleted unnecessary column created in the above cell\n",
    "books_df = books_df.drop(columns=['author_name_length'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration ratings data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df[~ratings_df['ISBN'].apply(is_valid_isbn)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checked ISBN in ratings df and there is 10159 rows with non valid ISBN, decided to drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df = ratings_df[ratings_df['ISBN'].apply(is_valid_isbn)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df[\"Book-Rating\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration users dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df['Age'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum and minimum of the values (0 and 244) in the age column indicate that the age column contains incorrect values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(users_df['Age'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df['Age'] = users_df['Age'].astype(pd.Int64Dtype())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df['Age'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df.loc[(users_df['Age'] < 6) | (users_df['Age'] > 99), 'Age'] = np.nan\n",
    "users_df['Age'] = users_df['Age'].astype(pd.Int64Dtype())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df[users_df['Age'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently ~40% do not seem to have age information filled in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge data frames and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if there are white spaces, HTML entits and HTML tags or double spaces\n",
    "def analyze_text_issues(text):\n",
    "    double_space = bool(re.search(r'  ', text))\n",
    "    html_tag = bool(re.search(r'<.*?>', text))\n",
    "    html_entity = text != html.unescape(text)\n",
    "    extra_whitespace = text != text.strip()\n",
    "\n",
    "    return double_space, html_tag, html_entity, extra_whitespace\n",
    "\n",
    "# Function which use function analyze_text_issues in columns\n",
    "def summarize_column_issues(df, columns_to_inspect):\n",
    "    summary = {}\n",
    "    for column in columns_to_inspect:\n",
    "        results = df[column].astype(str).dropna().apply(analyze_text_issues)\n",
    "        summary[column] = {\n",
    "            'double_spaces': results.apply(lambda x: x[0]).sum(),\n",
    "            'html_tags': results.apply(lambda x: x[1]).sum(),\n",
    "            'html_entities': results.apply(lambda x: x[2]).sum(),\n",
    "            'extra_whitespace': results.apply(lambda x: x[3]).sum()\n",
    "        }\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean data from white spaces, HTML entits and HTML tags or double spaces\n",
    "def clean_text_data(text):\n",
    "    previous_text = \"\"\n",
    "    while previous_text != text:\n",
    "        previous_text = text\n",
    "        text = re.sub(r'<.*?>', '', text)\n",
    "        text = text.replace('  ', ' ')\n",
    "        text = html.unescape(text)\n",
    "        text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge user and ratings info to get one dataframe with user-ratings info.\n",
    "user_ratings_df = pd.merge(ratings_df, users_df, on='User-ID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe user_ratings_df grouped by ISBN and aggregating the information about age, book rating and user-id in list\n",
    "# The final merge contains the main book dataframe with information about rating, age and user- id aggregated in list\n",
    "isbn_user_ratings = user_ratings_df.groupby('ISBN').agg({\n",
    "    'User-ID': list,\n",
    "    'Book-Rating': list,\n",
    "    'Age': list \n",
    "})\n",
    "merged_df = pd.merge(books_df, isbn_user_ratings, on='ISBN', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_check = ['Book-Title', 'Book-Author', 'Publisher']\n",
    "summary = summarize_column_issues(merged_df, columns_to_check)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function clean_text_data on columns 'Book-Title', 'Book-Author', 'Publisher'\n",
    "for column in ['Book-Title', 'Book-Author', 'Publisher']:\n",
    "    merged_df[column] = merged_df[column].astype(str).apply(clean_text_data)\n",
    "merged_df[['Book-Title', 'Book-Author', 'Publisher']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several columns have empty values, for example Year-Of-Publication, Publisher can be checked to get data from the API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a list of query where each element has 100 unique ISBN, which meets the conditions of the API.\n",
    "def create_list_query(merged_df):\n",
    "    unique_isbn = merged_df['ISBN'].unique()\n",
    "    isbn_list = unique_isbn.tolist()\n",
    "    chunked_isbn_list = [isbn_list[i:i + 100] for i in range(0, len(isbn_list), 100)]\n",
    "    list_query = []\n",
    "    for chunk in chunked_isbn_list:\n",
    "        isbn_str = \",\".join(f\"ISBN:{isbn}\" for isbn in chunk)\n",
    "        url = f\"https://openlibrary.org/api/books?bibkeys={isbn_str}&jscmd=details&format=json\"\n",
    "        list_query.append(url)\n",
    "    return list_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_query = create_list_query(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get data and continuosly save them in a new line in data.json file, if error will occure it will give the url and status code\n",
    "def get_data(list_query):\n",
    "    with open('data.json', 'a') as output_file:  \n",
    "        for url in list_query:\n",
    "                response = requests.get(url)\n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    json.dump(data, output_file)\n",
    "                    output_file.write('\\n')  # Newline character to separate JSON objects\n",
    "                else:\n",
    "                    print(f\"Request failed for {url}. Status code: {response.status_code}\")\n",
    "                time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_data(list_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_book_data(file_path):\n",
    "    all_books = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            json_obj = json.loads(line.strip())\n",
    "\n",
    "            for isbn, book_info in json_obj.items():\n",
    "                if isinstance(book_info, dict):\n",
    "                    details = book_info.get('details', {})\n",
    "                    \n",
    "                    if isinstance(details, dict):\n",
    "                        title = details.get('title')\n",
    "                        number_of_pages = details.get('number_of_pages')\n",
    "                        publish_date = details.get('publish_date')\n",
    "\n",
    "                        publishers = details.get('publishers', [])\n",
    "                        publisher = publishers[0] if publishers else None\n",
    "                        \n",
    "                        if isinstance(details.get('subjects', []), list):\n",
    "                            subjects = details.get('subjects', [])\n",
    "                        else:\n",
    "                            subjects = []\n",
    "\n",
    "                        if isinstance(details.get('genres', []), list):\n",
    "                            genres = details.get('genres', [])\n",
    "                        else:\n",
    "                            genres = []\n",
    "\n",
    "                        book_data = {\n",
    "                            'ISBN': isbn,\n",
    "                            'Title': title,\n",
    "                            'Number_of_Pages': number_of_pages,\n",
    "                            'Publisher': publisher,\n",
    "                            'publish_date': publish_date,\n",
    "                            'Subjects': ', '.join(subjects), \n",
    "                            'Genres': ', '.join(genres) \n",
    "                        }\n",
    "                        all_books.append(book_data)\n",
    "\n",
    "    return pd.DataFrame(all_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data.json' \n",
    "external_books_df = extract_book_data(file_path)\n",
    "external_books_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning from ISBN:\n",
    "external_books_df['ISBN'] = external_books_df['ISBN'].str.replace('ISBN:', '', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page number from float to int\n",
    "external_books_df['Number_of_Pages'] = external_books_df['Number_of_Pages'].astype(pd.Int64Dtype())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_check = ['Title', 'Subjects', 'Publisher', 'Genres']\n",
    "summary = summarize_column_issues(external_books_df, columns_to_check)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ['Title', 'Subjects', 'Publisher', 'Genres']:\n",
    "    external_books_df[column] = external_books_df[column].astype(str).apply(clean_text_data)\n",
    "external_books_df[['Title', 'Subjects', 'Publisher', 'Genres']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'publish_date' column to datetime and keeping only the information about year\n",
    "external_books_df['publish_date'] = pd.to_datetime(external_books_df['publish_date'], errors='coerce')\n",
    "external_books_df['year'] = external_books_df['publish_date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_books_df['year'] = external_books_df['publish_date'].dt.year\n",
    "\n",
    "# Drop the original 'publish_date' column as we now have the year\n",
    "external_books_df = external_books_df.drop('publish_date', axis=1)\n",
    "\n",
    "# Return the first few rows of the modified DataFrame to check the result\n",
    "external_books_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting year value to int\n",
    "external_books_df['year'] = external_books_df['year'].astype(pd.Int64Dtype())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling information about year which is present in external_book_df and missing in merged_df\n",
    "# It filled 3574 values\n",
    "comparison_df = pd.merge(merged_df[['ISBN', 'Year-Of-Publication']], external_books_df[['ISBN', 'year']], on='ISBN', how='outer', indicator=True)\n",
    "condition = (comparison_df['Year-Of-Publication'].isna()) & (comparison_df['year'].notna())\n",
    "isbns_to_update = comparison_df.loc[condition, 'ISBN']\n",
    "for isbn in isbns_to_update:\n",
    "    new_year = comparison_df.loc[comparison_df['ISBN'] == isbn, 'year'].iloc[0]\n",
    "    merged_df.loc[merged_df['ISBN'] == isbn, 'Year-Of-Publication'] = new_year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_genres = external_books_df['Genres'].unique()\n",
    "unique_genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing text \"etc., etc and .\", double spaces \n",
    "def clean_and_split_genres(genre):\n",
    "    if pd.isna(genre):\n",
    "        return genre \n",
    "    \n",
    "    cleaned_genre = re.sub(r'\\.', '', genre)\n",
    "    cleaned_genre = re.sub(r'\\betc\\b\\.?', '', cleaned_genre)\n",
    "    cleaned_genre = re.sub(r'\\s+', ' ', cleaned_genre)\n",
    "    split_genres = [g.strip() for g in cleaned_genre.split(',') if g.strip()]\n",
    "    \n",
    "    return split_genres\n",
    "external_books_df['Genres'] = external_books_df['Genres'].apply(clean_and_split_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_genres_count = external_books_df[external_books_df['Genres'].apply(lambda x: not x)].shape[0]\n",
    "empty_genres_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunatelly there are so many empty list for column 'Genres', that this columns can't be used.\n",
    "The column 'Subject' contains many key words which can help to reccomend books\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Subjects' entries into lists so it will be possible to apply function clean_and_split_list\n",
    "external_books_df['Subjects'] = external_books_df['Subjects'].apply(lambda x: [x] if isinstance(x, str) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_split_list(text_list):\n",
    "    # Initialize an empty list to store cleaned words\n",
    "    cleaned_words = []\n",
    "\n",
    "    # Check if the list is not empty or null\n",
    "\n",
    "    if text_list and isinstance(text_list, list):\n",
    "        for text in text_list:\n",
    "            if pd.isna(text) or not isinstance(text, str):\n",
    "                continue  # Skip non-string and NaN \n",
    "\n",
    "            text = text.lower()\n",
    "            cleaned_text = re.sub(r'[^a-z0-9\\s-]', ' ', text)  \n",
    "            cleaned_text = re.sub(r'(?<!\\w)-|-(?!\\w)', ' ', cleaned_text)\n",
    "            cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "\n",
    "            word_list = cleaned_text.split()\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            word_list = [word for word in word_list if word not in stop_words]\n",
    "\n",
    "            # Extend the main list with the cleaned words\n",
    "            cleaned_words.extend(word_list)\n",
    "\n",
    "    return cleaned_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to both 'Subjects' and 'Genres' columns\n",
    "external_books_df['Cleaned_Subjects'] = external_books_df['Subjects'].apply(clean_and_split_list)\n",
    "external_books_df['Cleaned_Genres'] = external_books_df['Genres'].apply(clean_and_split_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenated column Cleaned_Subjects and Cleaned_Genres in one list\n",
    "external_books_df['Concatenated_S_G'] = external_books_df['Cleaned_Subjects'] + external_books_df['Cleaned_Genres']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to lemmatize text\n",
    "def lemmatize_words(word_list):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # Lemmatize each word in the list\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in word_list]\n",
    "    return lemmatized_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'Concatenated_S_G' is a list of words, you would apply the function like this:\n",
    "external_books_df['Lemmatized_S_G'] = external_books_df['Concatenated_S_G'].apply(lemmatize_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to check which genres are in our data to specify the known genres \n",
    "# non_empty_genres = external_books_df['Cleaned_Genres'].dropna().loc[external_books_df['Cleaned_Genres'] != '']\n",
    "# unique_genres_set = set()\n",
    "# for genre_list in external_books_df['Cleaned_Genres']:\n",
    "#     if genre_list:  \n",
    "#         unique_genres_set.update(genre_list)\n",
    "# print(list(unique_genres_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_genres = [\n",
    "    'adventure', 'biography', 'fantasy', 'historical', 'horror', 'literary', 'mystery', \n",
    "    'mythology', 'non-fiction', 'philosophical', 'romance', 'satire', 'science', 'juvenile',\n",
    "    'thriller', 'western', 'young', 'action', 'drama', 'erotica', 'memoir', 'crime', \n",
    "    'dystopian', 'self-help', 'travel', 'guide', 'anthology', 'classic', 'comedy', \n",
    "    'psychological', 'suspense', 'tragedy', 'fairy', 'folklore', 'legend', 'narrative', \n",
    "    'periodical', 'political', 'realistic', 'reference', 'religion', 'short', 'superhero', \n",
    "    'supernatural', 'textbook', 'urban', 'utopian', 'war', 'absurdist', 'alternate', \n",
    "    'coming-of-age', 'cookbook', 'diary', 'encyclopedia', 'epic', 'experimental', 'fable', \n",
    "    'fan', 'gothic', 'graphic', 'hard-boiled', 'historiography', 'humor', 'lab', 'magical', \n",
    "    'paranormal', 'picaresque', 'post-apocalyptic', 'stream-of-consciousness', 'sword', \n",
    "    'true', 'vampire', 'visionary', 'whodunit', 'non-fiction', 'biology', 'music', 'guidebook',\n",
    "    'vocabularies', 'design', 'architecture', 'novela', 'archeology', 'tour', 'statistic',\n",
    "    'anecdotes', 'guidebook', 'manual', 'history', 'child', 'study', 'work', 'dictionaries', 'humor',\n",
    "    'handbook', 'pictorial', 'personal', 'poetry', 'interview', 'fiction', 'literature', 'guidebooks',\n",
    "    'social', 'detective', 'life', 'fictitious', 'art'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check for matched from list know_genres in df with list comprehension\n",
    "def find_genre_matches(lemmatized_genres, known_genres):\n",
    "    match = [genre for genre in known_genres if genre in lemmatized_genres]\n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created new column Category where are saved Category of book in list appling function find_genre_matches\n",
    "external_books_df['Category'] = external_books_df['Lemmatized_S_G'].apply(lambda row: find_genre_matches(row, known_genres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_books_df['Category'].apply(lambda x: len(x) == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "269364 - 117928\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(100/269364) * 151436"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_books_df.to_csv(\"test_external_books_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have created a new column named 'Category' that contains more accurate information regarding the categories of various books, presented in list format. This enhancement came after cleaning the 'Subject' and 'Genres' columns, followed by word splitting and the application of lemmatization techniques using a specific library. Subsequently, I established a list of prevalent book categories. Upon cross-verifying the presence of these categories within the text, I generated a new column. This column boasts a fill rate of approximately 56%. This new column will be used for book recommendation model more accorated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final processing of merged dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a new column for Publication Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new column named 'Publication-Range' whith a range of Year-Of-Publication by decade, it is by defualt left closed\n",
    "bins = list(range(1900, 2030, 10))  \n",
    "labels = [f\"{i}-{i+10}\" for i in bins[:-1]]\n",
    "\n",
    "merged_df['Publication-Range'] = pd.cut(merged_df['Year-Of-Publication'], bins=bins, labels=labels, right=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a new column mean rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert Book-Rating to list\n",
    "def convert_to_list(rating):\n",
    "    if isinstance(rating, list):\n",
    "        return rating\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['Book-Rating'] = merged_df['Book-Rating'].apply(convert_to_list)\n",
    "\n",
    "# Calculated the mean rating, taking into account empty lists and ensuring division by zero doesn't occur\n",
    "merged_df['Mean-Rating'] = merged_df['Book-Rating'].apply(lambda x: sum(x) / len(x) if x else float('nan'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the entire 'Mean-Rating' column to native Python data types\n",
    "merged_df['Mean-Rating'] = merged_df['Mean-Rating'].apply(lambda x: int(x) if isinstance(x, float) and x.is_integer() else x).astype(object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join column from external_books_df to final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extented merged_df of external_book_df on ISBN by selected column\n",
    "final_merge = pd.merge(merged_df, external_books_df[['ISBN','Category','Number_of_Pages']], on='ISBN', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unnecessary columns dropped\n",
    "final_merge = final_merge.drop(columns=['Year-Of-Publication'])\n",
    "final_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dealing with < NA > values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_na(value):\n",
    "    return None if pd.isna(value) else value\n",
    "\n",
    "final_merge[\"Age\"] = final_merge[\"Age\"].apply(lambda x: [replace_na(item) for item in x] if isinstance(x, list) else replace_na(x))\n",
    "final_merge[\"Number_of_Pages\"] = final_merge[\"Number_of_Pages\"].apply(lambda x: [replace_na(item) for item in x] if isinstance(x, list) else replace_na(x))\n",
    "final_merge[\"Book-Rating\"] = final_merge[\"Book-Rating\"].apply(lambda x: [replace_na(item) for item in x] if isinstance(x, list) else replace_na(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book Recommendation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reviewed_by_user(user_id, df):\n",
    "    # First, create a new dataframe by filtering rows that contain the user_id in the 'User-ID' lists.\n",
    "    df_reviewed_by_user = df[df['User-ID'].apply(lambda x: user_id in x if isinstance(x, list) else False)]\n",
    "    return df_reviewed_by_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_book_details(df):\n",
    "    columns_of_interest = ['ISBN', 'Publication-Range', 'Category', 'Book-Author', 'Mean-Rating', 'Publisher']\n",
    "    df_books_details = df[columns_of_interest].drop_duplicates(subset='ISBN')\n",
    "    return df_books_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get a dataframe of ISBN, User-ID and Book-Rating exploding the data from the function get_user_isbns\n",
    "def get_user_ratings(user_data, user_id):\n",
    "    user_data_exploded = user_data.explode('User-ID').reset_index(drop=True)\n",
    "    user_data_exploded['Book-Rating'] = user_data['Book-Rating'].explode().reset_index(drop=True)\n",
    "\n",
    "    filtered_data = user_data_exploded[user_data_exploded['User-ID'] == user_id]\n",
    "    user_rating = filtered_data[['ISBN', 'User-ID', 'Book-Rating']]\n",
    "    return user_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_users_books(user_rating, book_details):\n",
    "    \n",
    "    high_rated_books = user_rating[user_rating['Book-Rating'] >= 6]['ISBN']\n",
    "    low_rated_books = user_rating[user_rating['Book-Rating'] < 5]['ISBN']\n",
    "\n",
    "    high_rating = book_details[book_details['ISBN'].isin(high_rated_books)]\n",
    "    low_rating = book_details[book_details['ISBN'].isin(low_rated_books)]\n",
    "    return high_rating, low_rating  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content-Based Recommendation or Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = final_merge.copy()\n",
    "rec = rec[['Category', 'Publisher', 'Book-Author']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert any numerical values to strings and handle nulls\n",
    "rec['Category'] = rec['Category'].apply(lambda x: str(x) if not isinstance(x, str) else x).fillna('Unknown')\n",
    "rec['Publisher'] = rec['Publisher'].apply(lambda x: str(x) if not isinstance(x, str) else x).fillna('Unknown')\n",
    "rec['Book-Author'] = rec['Book-Author'].apply(lambda x: str(x) if not isinstance(x, str) else x).fillna('Unknown')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "# Define the size of the hash space and initialize the FeatureHasher\n",
    "n_features = 2**17  # You can adjust based on your system's capabilities\n",
    "hasher = FeatureHasher(n_features=n_features, input_type='string')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.sparse import save_npz, load_npz, hstack, vstack\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import FeatureHasher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory path within the current working directory or another writable location\n",
    "directory_path = 'hashed_batches'  # relative path indicating a subdirectory in the current working directory\n",
    "\n",
    "# Create a directory for intermediate files if it doesn't exist\n",
    "if not os.path.exists(directory_path):\n",
    "    os.makedirs(directory_path)\n",
    "\n",
    "# Set the batch size\n",
    "batch_size = 500  # Or another size fitting your memory capacity\n",
    "n_batches = len(rec) // batch_size + 1\n",
    "\n",
    "# Initialize a list to hold the results (sparse matrices)\n",
    "hashed_results = []\n",
    "\n",
    "for i in range(n_batches):\n",
    "    start_index = i * batch_size\n",
    "    end_index = (i + 1) * batch_size\n",
    "\n",
    "    # Extract the batch\n",
    "    batch = rec[start_index:end_index]\n",
    "\n",
    "    # Convert strings to lists of words for hashing\n",
    "    categories_iterable = [[item] for item in batch['Category'].tolist()]\n",
    "    publishers_iterable = [[item] for item in batch['Publisher'].tolist()]\n",
    "    authors_iterable = [[item] for item in batch['Book-Author'].tolist()]\n",
    "\n",
    "    # Apply hashing for each batch and keep the results as sparse matrices\n",
    "    hashed_category = hasher.transform(categories_iterable)\n",
    "    hashed_publisher = hasher.transform(publishers_iterable)\n",
    "    hashed_author = hasher.transform(authors_iterable)\n",
    "\n",
    "    # Combine hashed features for the batch (in sparse matrix format)\n",
    "    combined_features_batch = hstack([hashed_category, hashed_publisher, hashed_author])  # Stacking vertically\n",
    "\n",
    "    # Save each batch to disk instead of keeping in memory\n",
    "    batch_file_path = os.path.join(directory_path, f'batch_{i}.npz')  # safer with os.path.join\n",
    "    save_npz(batch_file_path, combined_features_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = 'hashed_batches' \n",
    "# Set the batch size\n",
    "batch_size = 500  # Or another size fitting your memory capacity\n",
    "n_batches = len(rec) // batch_size + 1\n",
    "\n",
    "# Initialize a list to hold the results (sparse matrices)\n",
    "hashed_results = []\n",
    "\n",
    "# After all batches are processed and saved, load them one at a time for concatenation\n",
    "for i in range(n_batches):\n",
    "    batch_file_path = os.path.join(directory_path, f'batch_{i}.npz')\n",
    "    batch_data = load_npz(batch_file_path)\n",
    "    hashed_results.append(batch_data)\n",
    "\n",
    "# Concatenate all batches to get the final feature set (as a sparse matrix)\n",
    "combined_features_sparse = vstack(hashed_results)  # This is still a sparse matrix\n",
    "\n",
    "# Now, you can proceed with your machine learning model training using combined_features_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_features_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def find_similar_books(book_id, X, book_ids, k, metric='cosine', show_distance=False):\n",
    "    \"\"\"\n",
    "    Find k similar books based on their vector representation in X.\n",
    "    \n",
    "    Parameters:\n",
    "    - book_id: The ID (ISBN) of the book of interest.\n",
    "    - X: The matrix (sparse or dense) representation of the books.\n",
    "    - book_ids: A list of book IDs in the same order as they appear in X.\n",
    "    - k: Number of similar books to find.\n",
    "    - metric: The distance metric to use. Default is 'cosine'.\n",
    "    - show_distance: Whether to show distance values. Default is False.\n",
    "    \n",
    "    Returns:\n",
    "    A list of k book IDs that are similar to the given book_id.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find the index of the book_id in the book_ids list\n",
    "    book_ind = book_ids.index(book_id)\n",
    "    \n",
    "    # Extract the vector representation of the book from X\n",
    "    book_vec = X[book_ind]\n",
    "    \n",
    "    # Initialize and fit the kNN model\n",
    "    kNN = NearestNeighbors(n_neighbors=k+1, algorithm=\"brute\", metric=metric)\n",
    "    kNN.fit(X)\n",
    "    \n",
    "    # Reshape the book vector and find its neighbors\n",
    "    book_vec = book_vec.reshape(1, -1)\n",
    "    neighbours = kNN.kneighbors(book_vec, return_distance=show_distance)\n",
    "    \n",
    "    # Extract the indices of the neighbours from the kNN result\n",
    "    neighbour_indices = neighbours[0].tolist()\n",
    "    \n",
    "    # Convert these indices back to book IDs\n",
    "    neighbour_ids = [book_ids[i] for i in neighbour_indices]\n",
    "    \n",
    "    # Remove the original book_id from the result\n",
    "    neighbour_ids.remove(book_id)\n",
    "\n",
    "    return neighbour_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 1\n",
    "df_reviewed_by_user = get_reviewed_by_user(user_id, final_merge)\n",
    "df_books_details = extract_book_details(df_reviewed_by_user)\n",
    "user_rating = get_user_ratings(df_reviewed_by_user, user_id)\n",
    "high_rating, low_rating  = categorize_users_books(user_rating, df_books_details)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_ids = high_rating['ISBN']\n",
    "book_ids.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isbns = final_merge['ISBN']\n",
    "isbn_list = isbns.to_list()\n",
    "book_id = \"0002005018\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary 'length' column\n",
    "final_merge['length'] = final_merge['Book-Rating'].apply(len)\n",
    "\n",
    "# Sort by 'Mean-Rating' and 'length'\n",
    "sorted_df = final_merge.sort_values(by=['Mean-Rating', 'length'], ascending=[False, False])\n",
    "\n",
    "# Drop the temporary 'length' column\n",
    "sorted_df = sorted_df.drop(columns=['length'])\n",
    "\n",
    "sorted_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendation(user_high_rating_list, general_recommended_books):\n",
    "    all_books = []\n",
    "    if len(user_high_rating_list) == 0:\n",
    "        \n",
    "        all_books = general_recommended_books['ISBN'].head(3).tolist()\n",
    "    else:    \n",
    "        all_books = all_books + find_similar_books(isbn, combined_features_sparse, isbn_list, 5)\n",
    "    return all_books        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_books = get_recommendation(book_ids, sorted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_books_information = final_merge[final_merge['ISBN'].isin(found_books)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_books_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_books = found_books_information.sort_values(by='Mean-Rating', ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
